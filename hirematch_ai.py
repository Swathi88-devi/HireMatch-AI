# -*- coding: utf-8 -*-
"""HireMatch-AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yCA4-NzzMb9FQVoFFmaC5ZOyn0IncWXi
"""

!pip install pandas scikit-learn streamlit python-docx PyPDF2 pdfplumber --quiet

import pandas as pd
import re
from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score

# For file handling
import os
from PyPDF2 import PdfReader
import docx

import zipfile
import os
zip_path = "/content/Resume_Dataset.zip"
extract_path = "/content/Resume_Dataset/extracted/"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# List files to check
os.listdir(extract_path)

import pandas as pd

csv_file = os.path.join(extract_path, "Resume.csv")

df = pd.read_csv(csv_file, encoding='utf-8-sig', quotechar='"')
print(df.columns)

# Remove BOM, quotes, spaces
df.columns = [c.strip().replace('"','').replace("√Ø¬ª¬ø","") for c in df.columns]

# Rename resume_text for easier access
df.rename(columns={'resume_text':'resume_clean'}, inplace=True)

print(df.columns)

import pandas as pd

df = pd.read_csv('/content/Resume_Dataset/extracted/Resume.csv', encoding='utf-8-sig')
df.head()

import json

with open("/content/skills_vocab.json", "r") as f:
    skills_vocab = set(json.load(f))

print("Skills loaded:", len(skills_vocab))

import pandas as pd

csv_file = "/content/Resume_Dataset/extracted/Resume.csv"

# Load as single column
df = pd.read_csv(csv_file, header=None, encoding='utf-8-sig')

# The first row is header
header = df.iloc[0,0].replace('"','').split(',')

# Remaining rows are data
df = df[1:]
df = df[0].str.split(',', expand=True)
df.columns = header

# Optional: reset index
df = df.reset_index(drop=True)

# Check columns
print(df.columns)

import re

# Clean text function
def clean_text(t):
    if pd.isna(t): return ""
    t = str(t)
    t = t.encode("ascii","ignore").decode()
    t = re.sub(r'http\S+|www\S+', ' ', t)
    t = re.sub(r'[\r\n\t]+', ' ', t)
    t = re.sub(r'[^a-zA-Z0-9 .,]', ' ', t)
    t = re.sub(r'\s+', ' ', t)
    return t.strip().lower()

# Extract skills based on vocab
def extract_skills(text, vocab):
    text = clean_text(text)
    return [skill for skill in vocab if skill.lower() in text.lower()]

df['skills_extracted'] = df['resume_text'].apply(lambda x: extract_skills(x, skills_vocab))

# Quick check
df[['resume_text','skills_extracted']].head()

from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# Encode categories
le = LabelEncoder()
df['category_encoded'] = le.fit_transform(df['category'])

# Multi-hot encode skills
mlb = MultiLabelBinarizer()
skills_matrix = mlb.fit_transform(df['skills_extracted'])
skills_df = pd.DataFrame(skills_matrix, columns=mlb.classes_)

# Combine features
df_final = pd.concat([df[["resume_text","category_encoded"]].reset_index(drop=True),
                      skills_df.reset_index(drop=True)], axis=1)

X = df_final.drop(columns=["resume_text","category_encoded"])
y = df_final["category_encoded"]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Random Forest
clf = RandomForestClassifier(n_estimators=200, random_state=42)
clf.fit(X_train, y_train)

print(" Model trained successfully")

from PyPDF2 import PdfReader
import docx

# Read PDF
def read_pdf(file_path):
    text = ""
    reader = PdfReader(file_path)
    for page in reader.pages:
        t = page.extract_text()
        if t: text += t + " "
    return text

# Read DOCX
def read_docx(file_path):
    doc = docx.Document(file_path)
    text = " ".join([p.text for p in doc.paragraphs])
    return text

!pip install streamlit pandas fuzzywuzzy python-Levenshtein pyngrok pdfplumber python-docx nltk --quiet
!python -m nltk.downloader punkt

import pdfplumber
from docx import Document
import re
import nltk
from nltk.tokenize import word_tokenize

# PDF text extraction
def read_pdf(file):
    text = ""
    with pdfplumber.open(file) as pdf:
        for page in pdf.pages:
            text += page.extract_text() + " "
    return text

# DOCX text extraction
def read_docx(file):
    doc = Document(file)
    text = " ".join([p.text for p in doc.paragraphs])
    return text

# Basic text cleaning
def clean_text(text):
    text = text.lower()
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^a-zA-Z0-9 ,]', '', text)
    return text

# Skill extraction
def extract_skills(text, skills_vocab):
    tokens = word_tokenize(text)
    skills = [token for token in tokens if token in skills_vocab]
    return list(set(skills))

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# from utils import read_pdf, read_docx, clean_text, extract_skills
# 
# st.title("AI Resume Screening Dashboard")
# 
# # Sample skills vocabulary
# skills_vocab = ["python","excel","sql","machine learning","nlp","communication"]
# 
# uploaded_file = st.file_uploader("Upload your Resume (PDF or DOCX)", type=["pdf","docx"])
# 
# if uploaded_file:
#     # Extract text
#     if uploaded_file.name.endswith(".pdf"):
#         resume_text = read_pdf(uploaded_file)
#     elif uploaded_file.name.endswith(".docx"):
#         resume_text = read_docx(uploaded_file)
# 
#     resume_clean = clean_text(resume_text)
#     skills = extract_skills(resume_clean, skills_vocab)
# 
#     st.subheader("Detected Skills:")
#     st.write(skills)
# 
#     st.subheader("Resume Text Preview:")
#     st.write(resume_clean[:500] + "...")
#

# Step 1: Install required packages
!pip install python-docx PyPDF2 nltk fuzzywuzzy python-Levenshtein --quiet

import nltk
nltk.download('punkt')

import json
import pandas as pd
import re
from fuzzywuzzy import fuzz
from docx import Document
import PyPDF2
from google.colab import files

# Step 2: Upload skills_vocab.json
print("Upload your skills_vocab.json")
uploaded = files.upload()
skills_vocab_file = list(uploaded.keys())[0]

with open(skills_vocab_file, 'r') as f:
    skills_vocab = json.load(f)

# Step 3: Helper functions
def read_pdf(file):
    pdf_reader = PyPDF2.PdfReader(file)
    text = ""
    for page in pdf_reader.pages:
        text += page.extract_text()
    return text

def read_docx(file):
    doc = Document(file)
    return "\n".join([para.text for para in doc.paragraphs])

def clean_text(text):
    text = text.lower()
    text = re.sub(r'\n', ' ', text)
    text = re.sub(r'[^a-zA-Z0-9,.\s]', '', text)
    return text

def extract_skills(text, skills_vocab):
    detected = []
    for skill in skills_vocab:
        if fuzz.partial_ratio(skill.lower(), text) > 80:
            detected.append(skill)
    return detected

# Step 4: Upload your resume
print("Upload your resume (PDF or DOCX)")
uploaded_resume = files.upload()
resume_file = list(uploaded_resume.keys())[0]

if resume_file.endswith(".pdf"):
    resume_text = read_pdf(resume_file)
elif resume_file.endswith(".docx"):
    resume_text = read_docx(resume_file)
else:
    raise ValueError("File must be PDF or DOCX")

# Step 5: Clean and extract skills
resume_clean = clean_text(resume_text)
skills_found = extract_skills(resume_clean, skills_vocab)

# Step 6: Show results
print("\n--- Resume Text Preview ---\n")
print(resume_text[:1000])  # Show first 1000 characters
print("\n--- Detected Skills ---\n")
print(skills_found)

# Interactive Resume Fit Checker
import json, re
import pdfplumber
import nltk
from nltk.tokenize import word_tokenize
from google.colab import files
from pdfminer.pdfdocument import PDFSyntaxError

nltk.download('punkt')
nltk.download('punkt_tab')


# -------------------------------
# Load skills vocabulary
# -------------------------------
with open('skills_vocab.json') as f:
    skills_vocab = json.load(f)

# -------------------------------
# Functions
# -------------------------------
def read_pdf(file_path):
    text = ''
    try:
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages:
                text += page.extract_text() + ' '
    except PDFSyntaxError:
        print(f"Error: Could not read PDF file '{file_path}'. It might be corrupted or not a valid PDF.")
        return None
    except Exception as e:
        print(f"An unexpected error occurred while reading PDF file '{file_path}': {e}")
        return None
    return text

def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def extract_skills(text, skills_vocab):
    words = set(word_tokenize(text))
    return [skill for skill in skills_vocab if skill.lower() in words]

# -------------------------------
# Job Description Input
# -------------------------------
jd_text = input("Paste Job Description here: ")
jd_clean = clean_text(jd_text)
jd_skills = extract_skills(jd_clean, skills_vocab)
print("\nJob Skills Required:", jd_skills)

# -------------------------------
# Upload Resume PDF(s)
# -------------------------------
uploaded = files.upload()  # Drag & drop PDF(s)

# -------------------------------
#  Process Resumes
# -------------------------------
for filename in uploaded:
    resume_text = read_pdf(filename)
    if resume_text is not None:
        resume_clean = clean_text(resume_text)
        resume_skills = extract_skills(resume_clean, skills_vocab)

        matched_skills = set(resume_skills).intersection(set(jd_skills))
        fit_score = len(matched_skills) / len(jd_skills) * 100 if jd_skills else 0

        print("\nResume:", filename)
        print("Detected Skills:", resume_skills)
        print("Matched Skills:", list(matched_skills))
        print("Missing Skills:", list(set(jd_skills) - set(resume_skills)))
        print("Resume Fit %: {:.2f}%".format(fit_score))
    else:
        print(f"Skipping analysis for file '{filename}' due to reading error.")

!pip install streamlit pdfplumber python-docx nltk

import nltk
nltk.download('punkt')

!pip install pdfplumber

!pip install python-docx

!pip install pdfplumber python-docx gradio nltk

import pdfplumber
from docx import Document
print("All libraries loaded successfully!")

# =========================================
#  HireMatch-AI ‚Äî AI Resume Fit Checker (Auto Skill Extraction)
# =========================================

import os
import re
import pdfplumber
from docx import Document
import gradio as gr
import nltk
from nltk.corpus import stopwords
from sentence_transformers import SentenceTransformer, util
from transformers import pipeline
import plotly.graph_objects as go

# ----------------------------
# Setup
# ----------------------------
nltk.download('stopwords', quiet=True)
stop_words = set(stopwords.words('english'))

# Load AI models
print("Loading models... (this may take 1-2 minutes the first time)")
ner_model = pipeline("ner", model="dslim/bert-base-NER", aggregation_strategy="simple")
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
print("‚úÖ Models loaded successfully!")

# ----------------------------
# Extract text from file
# ----------------------------
def extract_text_from_file(file):
    text = ""
    if file.name.endswith(".pdf"):
        with pdfplumber.open(file.name) as pdf:
            for page in pdf.pages:
                text += page.extract_text() or ""
    elif file.name.endswith(".docx"):
        doc = Document(file.name)
        for para in doc.paragraphs:
            text += para.text + "\n"
    else:
        raise ValueError("Unsupported file type. Please upload PDF or DOCX.")
    return text.strip()

# ----------------------------
# Clean text
# ----------------------------
def clean_text(text):
    text = text.lower()
    text = re.sub(r"[^a-z0-9\s]", " ", text)
    return re.sub(r"\s+", " ", text).strip()

# ----------------------------
# Auto Skill Extraction using NER
# ----------------------------
def extract_skills_auto(text):
    entities = ner_model(text)
    skills = [e['word'] for e in entities if e['entity_group'] in ['ORG', 'MISC']]
    # Remove duplicates and clean
    skills = list(set([s.lower() for s in skills if len(s) > 2]))
    return skills

# ----------------------------
# Semantic Skills Scoring
# ----------------------------
def get_skills_score_semantic(resume_text, jd_text, similarity_threshold=0.7):
    resume_skills = extract_skills_auto(resume_text)
    jd_skills = extract_skills_auto(jd_text)

    matched_skills = set(resume_skills).intersection(set(jd_skills))
    missing_skills = set(jd_skills) - set(resume_skills)

    # Optional semantic similarity boost
    if resume_skills and jd_skills:
        resume_embeddings = embedding_model.encode(resume_skills, convert_to_tensor=True)
        jd_embeddings = embedding_model.encode(jd_skills, convert_to_tensor=True)
        cos_sim = util.cos_sim(resume_embeddings, jd_embeddings)

        for i, res_skill in enumerate(resume_skills):
            for j, jd_skill in enumerate(jd_skills):
                if cos_sim[i][j] > similarity_threshold:
                    matched_skills.add(jd_skill)

    score = len(matched_skills) / (len(jd_skills) or 1)
    return round(score, 2), matched_skills, missing_skills

# ----------------------------
# ATS Scoring System
# ----------------------------
def ats_experience_score(resume_text, jd_text):
    years = re.findall(r'(\d+)\s+years?', resume_text.lower())
    resume_years = max([int(y) for y in years]) if years else 0
    jd_years = re.findall(r'(\d+)\+?\s+years?', jd_text.lower())
    jd_required = int(jd_years[0]) if jd_years else 0
    return round(min(resume_years / jd_required, 1), 2) if jd_required else 0

def ats_format_score(resume_text):
    headings = ["experience", "education", "skills", "projects"]
    heading_score = sum(1 for h in headings if h in resume_text.lower()) / len(headings)
    bullets = len(re.findall(r'‚Ä¢|\*|-', resume_text))
    bullet_score = 1 if bullets >= 3 else 0.5
    return round(0.7 * heading_score + 0.3 * bullet_score, 2)

def ats_total_score(resume_text, jd_text, skills_score):
    exp_score = ats_experience_score(resume_text, jd_text)
    fmt_score = ats_format_score(resume_text)
    total = round((skills_score * 0.6) + (exp_score * 0.3) + (fmt_score * 0.1), 2)
    breakdown = {"skills": skills_score, "experience": exp_score, "formatting": fmt_score}
    return total, breakdown

# ----------------------------
# Dashboard Visualization
# ----------------------------
def create_dashboard(breakdown):
    categories = list(breakdown.keys())
    values = [v * 100 for v in breakdown.values()]
    fig = go.Figure([go.Bar(x=categories, y=values, marker_color='teal')])
    fig.update_layout(title="Resume ATS Sub-Scores", yaxis=dict(title="Score (%)", range=[0, 100]),
                      xaxis=dict(title="Factors"), template="plotly_white")
    return fig

# ----------------------------
# Main Analysis
# ----------------------------
def analyze_resume_and_jd(resume_file, resume_text, jd_text):
    try:
        if resume_file:
            resume_content = extract_text_from_file(resume_file)
        elif resume_text:
            resume_content = resume_text
        else:
            return "‚ùå Please upload or paste your resume.", None, None, None, None, None

        if not jd_text:
            return "‚ùå Please paste a Job Description.", None, None, None, None, None

        skills_score, matched_skills, missing_skills = get_skills_score_semantic(resume_content, jd_text)
        total_score, breakdown = ats_total_score(resume_content, jd_text, skills_score)
        dashboard = create_dashboard(breakdown)

        return (
            resume_content[:2000] + "..." if len(resume_content) > 2000 else resume_content,
            f"ATS Total Score: {total_score*100:.1f}%",
            f"Matched Skills: {', '.join(matched_skills) or 'None'}",
            f"Missing Skills: {', '.join(missing_skills) or 'None'}",
            f"Sub-scores ‚Üí Skills: {breakdown['skills']*100:.0f}%, Exp: {breakdown['experience']*100:.0f}%, Format: {breakdown['formatting']*100:.0f}%",
            dashboard
        )
    except Exception as e:
        return f"‚ö†Ô∏è Error: {str(e)}", None, None, None, None, None

# ----------------------------
# Gradio Interface
# ----------------------------
with gr.Blocks() as demo:
    gr.Markdown("## ü§ñ HireMatch-AI ‚Äî Auto Skill Extraction + Dynamic ATS Dashboard")

    with gr.Row():
        with gr.Column():
            resume_file = gr.File(label="üìÅ Upload Resume (PDF/DOCX)")
            resume_text = gr.Textbox(lines=5, placeholder="Or paste your resume text...", label="Or paste resume text")
            jd_text = gr.Textbox(lines=5, placeholder="Paste Job Description (JD)...", label="Job Description (JD)")
            analyze_button = gr.Button("üöÄ Analyze Resume & JD")

        with gr.Column():
            resume_preview = gr.Textbox(label="Resume Preview")
            total_score = gr.Textbox(label="ATS Total Score")
            matched_box = gr.Textbox(label="Matched Skills")
            missing_box = gr.Textbox(label="Missing Skills")
            subscores_box = gr.Textbox(label="Detailed Sub-Scores")
            dashboard_plot = gr.Plot(label="ATS Score Dashboard")

    analyze_button.click(
        analyze_resume_and_jd,
        inputs=[resume_file, resume_text, jd_text],
        outputs=[resume_preview, total_score, matched_box, missing_box, subscores_box, dashboard_plot]
    )

# -------- Bind to Render or Hugging Face --------
if __name__ == "__main__":
    port = int(os.environ.get("PORT", 10000))
    demo.launch(server_name="0.0.0.0", server_port=port, share=False)